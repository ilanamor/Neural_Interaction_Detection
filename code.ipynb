{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "e663a9d1dfe83e5cac79d069805cc52ec2b4f7c2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import bisect\n",
    "import operator\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d9189e5fd95b383a44ea2ba1fb41fb832086a76"
   },
   "source": [
    "## Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "cf0aef4206b2b54058dadd132d2202b9c76be94e"
   },
   "outputs": [],
   "source": [
    "use_main_effect_nets = True # toggle this to use \"main effect\" nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "a51d73018bd2f723fea852ec901b543c245484b1"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 200\n",
    "batch_size = 100\n",
    "display_step = 100\n",
    "l1_const = 5e-5\n",
    "num_samples = 30000 #30k datapoints, split 1/3-1/3-1/3\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 140 # 1st layer number of neurons\n",
    "n_hidden_2 = 100 # 2nd layer number of neurons\n",
    "n_hidden_3 = 60 # 3rd \"\n",
    "n_hidden_4 = 20 # 4th \"\n",
    "n_hidden_uni = 10\n",
    "num_input = 10 # simple synthetic example input dimension\n",
    "num_output = 1 # regression or classification output dimension\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_output])\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "# Interaction data generator\n",
    "def synth_func(x):\n",
    "    interaction1 = np.exp(np.fabs(x[:,0]-x[:,1]))                        \n",
    "    interaction2 = np.fabs(x[:,1]*x[:,2])  \n",
    "    interaction3 = -1*np.power(np.power(x[:,2],2),np.fabs(x[:,3])) \n",
    "    interaction4 = np.power(x[:,0]*x[:,3],2)\n",
    "    interaction5 = np.log(np.power(x[:,3],2) + np.power(x[:,4],2) + np.power(x[:,6],2) + np.power(x[:,7],2))\n",
    "    main_effects = x[:,8] + 1/(1+np.power(x[:,9],2))\n",
    "\n",
    "    y =         interaction1 + interaction2 + interaction3 + interaction4 + interaction5 + main_effects\n",
    "    #ground truth:  {1,2}         {2,3}          {3,4}          {1,4}        {4,5,7,8}\n",
    "    return y\n",
    "\n",
    "def gen_synth_data():\n",
    "    X = np.random.uniform(low=-1, high=1, size=(num_samples,10))\n",
    "    Y = np.expand_dims(synth_func(X),axis=1)\n",
    "    \n",
    "    np_array = np.concatenate((X, Y), axis=1)\n",
    "    df = pd.DataFrame(np_array)\n",
    "    df.to_csv(\"synthetic.csv\", header=False, index=False)\n",
    "    \n",
    "    a = num_samples//3\n",
    "    b = 2*num_samples//3\n",
    "    \n",
    "    tr_x, va_x, te_x = X[:a], X[a:b], X[b:]\n",
    "    tr_y, va_y, te_y = Y[:a], Y[a:b], Y[b:]\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_x.fit(tr_x)\n",
    "    scaler_y.fit(tr_y)\n",
    "\n",
    "    tr_x, va_x, te_x = scaler_x.transform(tr_x), scaler_x.transform(va_x), scaler_x.transform(te_x)\n",
    "    tr_y, va_y, te_y = scaler_y.transform(tr_y), scaler_y.transform(va_y), scaler_y.transform(te_y)\n",
    "    return tr_x, va_x, te_x, tr_y, va_y, te_y\n",
    "\n",
    "# Get data\n",
    "tr_x, va_x, te_x, tr_y, va_y, te_y = gen_synth_data()\n",
    "tr_size = tr_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "32cf88d76e69656fc09ff1bc6ef42e4deed71740"
   },
   "outputs": [],
   "source": [
    "# access weights & biases\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal([num_input, n_hidden_1], 0, 0.1)),\n",
    "    'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], 0, 0.1)),\n",
    "    'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3], 0, 0.1)),\n",
    "    'h4': tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4], 0, 0.1)),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden_4, num_output], 0, 0.1))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.truncated_normal([n_hidden_1], 0, 0.1)),\n",
    "    'b2': tf.Variable(tf.truncated_normal([n_hidden_2], 0, 0.1)),\n",
    "    'b3': tf.Variable(tf.truncated_normal([n_hidden_3], 0, 0.1)),\n",
    "    'b4': tf.Variable(tf.truncated_normal([n_hidden_4], 0, 0.1)),\n",
    "    'out': tf.Variable(tf.truncated_normal([num_output], 0, 0.1))\n",
    "}\n",
    "\n",
    "def get_weights_uninet():\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([1, n_hidden_uni], 0, 0.1)),\n",
    "        'h2': tf.Variable(tf.truncated_normal([n_hidden_uni, n_hidden_uni], 0, 0.1)),\n",
    "        'h3': tf.Variable(tf.truncated_normal([n_hidden_uni, n_hidden_uni], 0, 0.1)),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_uni, num_output], 0, 0.1))\n",
    "    }\n",
    "    return weights\n",
    "\n",
    "def get_biases_uninet():\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.truncated_normal([n_hidden_uni], 0, 0.1)),\n",
    "        'b2': tf.Variable(tf.truncated_normal([n_hidden_uni], 0, 0.1)),\n",
    "        'b3': tf.Variable(tf.truncated_normal([n_hidden_uni], 0, 0.1))\n",
    "    }\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "9d2df2c52cdd3c931665def2f71434edc06e3d9a"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def normal_neural_net(x, weights, biases):\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
    "    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['h3']), biases['b3']))\n",
    "    layer_4 = tf.nn.relu(tf.add(tf.matmul(layer_3, weights['h4']), biases['b4']))    \n",
    "    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "def main_effect_net(x, weights, biases):\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
    "    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['h3']), biases['b3']))    \n",
    "    out_layer = tf.matmul(layer_3, weights['out'])\n",
    "    return out_layer\n",
    "\n",
    "# L1 regularizer\n",
    "def l1_norm(a): return tf.reduce_sum(tf.abs(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "360ee2bc4c880cb73872703c5f96080980c68cdb"
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "net = normal_neural_net(X, weights, biases)\n",
    "\n",
    "if use_main_effect_nets:  \n",
    "    me_nets = []\n",
    "    for x_i in range(num_input):\n",
    "        me_net = main_effect_net(tf.expand_dims(X[:,x_i],1), get_weights_uninet(), get_biases_uninet())\n",
    "        me_nets.append(me_net)\n",
    "    net = net + sum(me_nets)\n",
    "\n",
    "# Define optimizer\n",
    "loss_op = tf.losses.mean_squared_error(labels=Y, predictions=net)\n",
    "# loss_op = tf.sigmoid_cross_entropy_with_logits(labels=Y,logits=net) # use this in the case of binary classification\n",
    "sum_l1 = tf.reduce_sum([l1_norm(weights[k]) for k in weights])\n",
    "loss_w_reg_op = loss_op + l1_const*sum_l1 \n",
    "\n",
    "batch = tf.Variable(0)\n",
    "decaying_learning_rate = tf.train.exponential_decay(learning_rate, batch*batch_size, tr_size, 0.95, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=decaying_learning_rate).minimize(loss_w_reg_op, global_step=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true,
    "_uuid": "a300a0b8e819ba2a1bff7f613cb4a5bda73ccc0d"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "n_batches = tr_size//batch_size\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.25\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Initialized')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    batch_order = list(range(n_batches))\n",
    "    np.random.shuffle(batch_order)\n",
    "\n",
    "    for i in batch_order:\n",
    "        batch_x = tr_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_y = tr_y[i*batch_size:(i+1)*batch_size]\n",
    "        _, lr = sess.run([optimizer,decaying_learning_rate], feed_dict={X:batch_x, Y:batch_y})\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        tr_mse = sess.run(loss_op, feed_dict={X:tr_x, Y:tr_y})\n",
    "        va_mse = sess.run(loss_op, feed_dict={X:va_x, Y:va_y})\n",
    "        te_mse = sess.run(loss_op, feed_dict={X:te_x, Y:te_y})\n",
    "        print('Epoch', epoch+1)\n",
    "        print('\\t','train rmse', math.sqrt(tr_mse), 'val rmse', math.sqrt(va_mse), 'test rmse', math.sqrt(te_mse))\n",
    "        print('\\t','learning rate', lr)\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dcf730c071ca6db5bf35f30f5f6b81bc33c7150a"
   },
   "source": [
    "## Interpret Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "6b40ec4dd7816485b8e3777409ca659ca49a76f0"
   },
   "outputs": [],
   "source": [
    "def preprocess_weights(w_dict):\n",
    "    hidden_layers = [int(layer[1:]) for layer in w_dict.keys() if layer.startswith('h')]\n",
    "    output_h = ['h' + str(x) for x in range(max(hidden_layers),1,-1)]\n",
    "    w_agg = np.abs(w_dict['out'])\n",
    "    w_h1 = np.abs(w_dict['h1'])\n",
    "\n",
    "    for h in output_h:\n",
    "        w_agg = np.matmul( np.abs(w_dict[h]), w_agg)\n",
    "\n",
    "    return w_h1, w_agg \n",
    "\n",
    "def get_interaction_ranking(w_dict):\n",
    "    xdim = w_dict['h1'].shape[0]\n",
    "    w_h1, w_agg = preprocess_weights(w_dict)\n",
    "        \n",
    "    # rank interactions\n",
    "    interaction_strengths = dict()\n",
    "\n",
    "    for i in range(len(w_agg)):\n",
    "        sorted_fweights = sorted(enumerate(w_h1[:,i]), key=lambda x:x[1], reverse = True)\n",
    "        interaction_candidate = []\n",
    "        weight_list = []       \n",
    "        for j in range(len(w_h1)):\n",
    "            bisect.insort(interaction_candidate, sorted_fweights[j][0]+1)\n",
    "            weight_list.append(sorted_fweights[j][1])\n",
    "            if len(interaction_candidate) == 1:\n",
    "                continue\n",
    "            interaction_tup = tuple(interaction_candidate)\n",
    "            if interaction_tup not in interaction_strengths:\n",
    "                interaction_strengths[interaction_tup] = 0\n",
    "            inter_agg = min(weight_list)      \n",
    "            interaction_strengths[interaction_tup] += np.abs(inter_agg*np.sum(w_agg[i]))\n",
    "        \n",
    "    interaction_sorted = sorted(interaction_strengths.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    # forward prune the ranking of redundant interactions\n",
    "    interaction_ranking_pruned = []\n",
    "    existing_largest = []\n",
    "    for i, inter in enumerate(interaction_sorted):\n",
    "        if len(interaction_ranking_pruned) > 20000: break\n",
    "        skip = False\n",
    "        indices_to_remove = set()\n",
    "        for inter2_i, inter2 in enumerate(existing_largest):\n",
    "            # if this is not the existing largest\n",
    "            if set(inter[0]) < set(inter2[0]):\n",
    "                skip = True\n",
    "                break\n",
    "            # if this is larger, then need to recall this index later to remove it from existing_largest\n",
    "            if set(inter[0]) > set(inter2[0]):\n",
    "                indices_to_remove.add(inter2_i)\n",
    "        if skip:\n",
    "            assert len(indices_to_remove) == 0\n",
    "            continue\n",
    "        prevlen = len(existing_largest)\n",
    "        existing_largest[:] = [el for el_i, el in enumerate(existing_largest) if el_i not in indices_to_remove]\n",
    "        existing_largest.append(inter)\n",
    "        interaction_ranking_pruned.append((inter[0], inter[1]))\n",
    "\n",
    "        curlen = len(existing_largest)\n",
    "\n",
    "    return interaction_ranking_pruned\n",
    "\n",
    "def get_pairwise_ranking(w_dict):\n",
    "    xdim = w_dict['h1'].shape[0]\n",
    "    w_h1, w_agg = preprocess_weights(w_dict)\n",
    "\n",
    "    input_range = range(1,xdim+1)\n",
    "    pairs = [(xa,yb) for xa in input_range for yb in input_range if xa != yb]\n",
    "    for entry in pairs:\n",
    "        if (entry[1], entry[0]) in pairs:\n",
    "            pairs.remove((entry[1],entry[0]))\n",
    "\n",
    "    pairwise_strengths = []\n",
    "    for pair in pairs:\n",
    "        a = pair[0]\n",
    "        b = pair[1]\n",
    "        wa = w_h1[a-1].reshape(w_h1[a-1].shape[0],1)\n",
    "        wb = w_h1[b-1].reshape(w_h1[b-1].shape[0],1)\n",
    "        wz = np.abs(np.minimum(wa , wb))*w_agg\n",
    "        cab = np.sum(np.abs(wz))\n",
    "        pairwise_strengths.append((pair, cab))\n",
    "#     list(zip(pairs, pairwise_strengths))\n",
    "\n",
    "    pairwise_ranking = sorted(pairwise_strengths,key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    return pairwise_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "9594507f8ab366ec2f816ccfd4e3d2077be94d76"
   },
   "outputs": [],
   "source": [
    "w_dict = sess.run(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52f63b9c7ccc223a0bcf4d488d10fd5b3fa88e0e"
   },
   "source": [
    "## Get Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "e48382c0ea1966e08743426763ead2c4df0829c9"
   },
   "outputs": [],
   "source": [
    "# Variable-Order Interaction Ranking\n",
    "get_interaction_ranking(w_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "f28c2998165a2990eda98a7497b7469afd6782e8"
   },
   "outputs": [],
   "source": [
    "# Pairwise Interaction Ranking\n",
    "get_pairwise_ranking(w_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "09d7b39481bb9b110185bdd15964ce6cb7652d09"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
